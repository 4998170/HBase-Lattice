#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article
\begin_preamble
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\usepackage{fullpage}
\usepackage{color}
\definecolor{tentative}{rgb}{0.4,0.6,0.7}
\definecolor{comment}{rgb}{0.7,0.4,0.4}
\end_preamble
\use_default_options true
\begin_modules
knitr
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman lmodern
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder false
\pdf_colorlinks false
\pdf_backref false
\pdf_pdfusetitle true
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation skip
\defskip medskip
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
HBase Lattice Quick Start
\end_layout

\begin_layout Author
Dmitriy Lyubimov
\begin_inset Newline newline
\end_inset


\size small
\emph on
dlyubimov at apache dot org
\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\begin_inset Newpage cleardoublepage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

<<knitrInit,echo=F,results='hide',warning=F,message=F>>=
\end_layout

\begin_layout Plain Layout

library(hblr)
\end_layout

\begin_layout Plain Layout

library(ggplot2)
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

@
\end_layout

\end_inset


\end_layout

\begin_layout Section
What it is 
\end_layout

\begin_layout Standard
HBase Lattice is an attempt at a big data BI solution.
 Namely, it is an attempt at implementation of HBase-based incremental OLAP-ish
 cubes.
 
\end_layout

\begin_layout Standard
Like some MOLAP and ROLAP solutions, HBase-Lattice copes with aggregate
 queries by prebuilding certain cuboids in a cube lattice model.
 Hence, the name.
\end_layout

\begin_layout Standard
Support of OLAP features (in MDX sense) is fairly limited, but being a big
 data solution, the project does not limit itself just to traditional OLAP
 feature set either.
 E.g.
 there's a way to run and consume HBL query as a locality-aware distributed
 MapReduce job, thus enabling fast export/processing of the cube data.
 Similarly, cube update is also a distributed job.
 Bottom line, I think it is good to be fairly open-minded in scope definition
 within a big data environment in the sense that some things in a standard
 are difficult to achieve and can be thrown away while some 
\begin_inset Quotes eld
\end_inset

nonstandard
\begin_inset Quotes erd
\end_inset

 features may become quite powerful enablers in the world of Big Data.
 I think this is approximately the same reason why the Hive project doesn't
 follow the exact set of SQL standard either.
\end_layout

\begin_layout Standard
Querying data is available as an API, or HBL query dialect and at the moment
 such clients are available for Java, MapReduce and R environments.
\end_layout

\begin_layout Standard

\series bold
Notations.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

{
\backslash
color{tentative}
\end_layout

\end_inset

Tentative or experimental material
\begin_inset ERT
status open

\begin_layout Plain Layout

}
\end_layout

\end_inset


\begin_inset Newline newline
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

{
\backslash
color{comment}
\end_layout

\end_inset

Comment
\begin_inset ERT
status open

\begin_layout Plain Layout

}
\end_layout

\end_inset


\strikeout on

\begin_inset Newline newline
\end_inset

Tentative deletion
\end_layout

\begin_layout Section
Motivation
\end_layout

\begin_layout Itemize

\series bold
In continuation of 
\begin_inset Quotes eld
\end_inset

Cassandra is OLTP, HBase is OLAP
\begin_inset Quotes erd
\end_inset

 mantra
\series default
.
 HBase is not really an OLAP service out of the door.
 It doesn't support cube models directly.
 There's no query language to use.
 There's no predefined way to update a cube.
 There're no concepts of dimension, hierarchy, measure and fact streams.
\end_layout

\begin_layout Itemize

\series bold
Big underlying fact stream
\series default
.
 Billions, perhaps trillions of facts a day to process which we want to
 cope with by parallelizing the compilation with the help of MapReduce.
 We also want to scale storage and querying capacity along with it horizontally
 on demand.
\end_layout

\begin_layout Itemize

\series bold
Low query TTLB
\series default
 (especially on Time Series data).
 Our goal was to answer queries over any period of time and whatever other
 slice specifications very quickly with a single hbase table access and
 a very limited amount of iterations in a scan.
 (TTLB <~ 1ms for the scanning code itself, +whatever overhead of currently
 configured HBase cache, + whatever network overhead).
\end_layout

\begin_layout Itemize

\series bold
Next to realtime data availability for querying
\series default
.
 Use of incremental updates to cuboid projections in the lattice means there's
 no need to recompile the whole cube for the past 90 days or whatever.
 Thus, cube data is not an immutable entity.
 New fact data becomes available within single number of minutes after the
 fact actually happened, as soon as incremental compiler iteration is complete.
 Once compiled, the data remains continuously available unless thrown out
 by HBase during compaction given specified projection TTL parameter.
\end_layout

\begin_layout Itemize

\series bold
Keep stuff within same ecosystem.
 
\series default
Another motivation is to be able to do things within the same resource space
 of HDFS and HBASE one has already invested in.
 While there are definitely other tools out there to try with the same,
 if not greater, capabilities (MongoDB comes to mind), those tools would
 perhaps require their own distinct environment (resources) and perhaps
 bulk data transfer and import.
\end_layout

\begin_layout Section
Notable differentiating facts
\end_layout

\begin_layout Paragraph
Compiler is a Pig codegen.
\end_layout

\begin_layout Standard
The compiler component generates a Pig script at runtime based on current
 specification of the model.
 
\end_layout

\begin_layout Standard
MapReduce compiler allows to do distributed aggregation of the data before
 making updates to HBase-stored cuboids.
 
\end_layout

\begin_layout Standard
It is possible to script out some additional distributed processing before
 (or in the same time as) passing the data to compilation.
\end_layout

\begin_layout Paragraph
Incremental data addition to the cube (aka 
\begin_inset Quotes eld
\end_inset

compilation
\begin_inset Quotes erd
\end_inset

).
 
\end_layout

\begin_layout Standard
Low latency for cube update cycles.
 (as often as compiler Pig job can be complete).
\end_layout

\begin_layout Paragraph
Custom HBase filtering for querying.
\end_layout

\begin_layout Standard
A special custom hbase filter is used to allow to skip over the rows we
 are not really interested in during composite key scan, so the scan iterations
 keep going over mostly relevant facts only and can skip over significant
 portion of the data between first and last scan rows.
\end_layout

\begin_layout Paragraph
Four different ways to query the cube data.
 
\end_layout

\begin_layout Standard
Querying is available via:
\end_layout

\begin_layout Itemize
Declarative Java client
\end_layout

\begin_layout Itemize
HBL query language dialect
\end_layout

\begin_layout Itemize
\begin_inset ERT
status open

\begin_layout Plain Layout

{
\backslash
color{tentative}
\end_layout

\end_inset

MapReduce distributed query (input format) for batch consumption
\begin_inset ERT
status open

\begin_layout Plain Layout

}
\end_layout

\end_inset


\end_layout

\begin_layout Itemize
R package for running hbl queries in R.
\end_layout

\begin_layout Paragraph
No fact table, no facts kept around.
\end_layout

\begin_layout Standard
We don't keep individual facts around.
 Unlike perhaps with some other approaches, there's no level of indirection
 to query the fact table.
 All projection data is right there, in a cuboid table.
 This provides 2 major benefits: 
\end_layout

\begin_layout Itemize
Low query TTLBs.
 If we are hitting cuboid with precompiled aggregate results, we only need
 to scan a handful of items per request.
 
\end_layout

\begin_layout Itemize
Don't need the space to keep all original facts.
 Depending on the definition of dimensions and hierarchies and the nature
 of incoming fact streams, the space required to keep aggregated projections
 may require several orders of magnitude less space than the original fact
 stream.
\end_layout

\begin_layout Standard
The tradeoff is obviously in that one cannot query individual fact datum.
 It is assumed that facts are kept somewhere else outside HBL tables (and
 they usually are, so no need to mandate data duplication in HBL).
\end_layout

\begin_layout Paragraph
What cuboids are to be compiled is specified manually.
\end_layout

\begin_layout Standard
In the interest of keeping things simple, the model specification explicitly
 lists all cuboids to compile in the cube.
 Consequently, not all aggregated groups are avaialable for querying.
 Working out which cuboids to compile is similar to process where DBA tries
 to figure out which indices to deploy based on use patterns.
 Optimizer selects most optimal (from its point of view) aggregation to
 use for a query automatically.
 Aggregate queries can use a range cuboids but the best performance is achieved
 whenever grouping fits the declared cuboid aggregating dimensions.
\end_layout

\begin_layout Paragraph
API to define new dimensions and aggregate functions.
\end_layout

\begin_layout Standard
Addition of custom dimension or measure type, or aggregate functions, is
 fairly easy and straightforward.
\end_layout

\begin_layout Section
Quick Howto
\end_layout

\begin_layout Subsection
Overview of the dev workflow
\begin_inset CommandInset label
LatexCommand label
name "sub:Overview-of-the"

\end_inset


\end_layout

\begin_layout Standard
The flow is as follows: 
\end_layout

\begin_layout Enumerate
Define cube model.
\end_layout

\begin_layout Enumerate
Deploy/update model to HBL system table in HBase.
 
\end_layout

\begin_layout Enumerate
Generate compiler script using compiler bean
\end_layout

\begin_layout Enumerate
Run incremental compiler cycle on a next portion of a fact stream.
\end_layout

\begin_layout Enumerate
Repeat step 4 as many times as needed.
\end_layout

\begin_layout Enumerate
At any moment after (2) querying the cube is enabled.
\end_layout

\begin_layout Standard
Step 1-2 may be repeated as many times as needed.
 Step 2 requires script update (step 3) in order for the changes to take
 effect.
 The changes committed in step (2) are effective immediately for step 6
 activities (almost; hbl client actually caches the models client side after
 a first query to a cube.
 So Hbl client interface must be re-created if reused in the client.
 Perhaps we could implement api to reset the model cache and force the reload).
\end_layout

\begin_layout Standard
Two fundamental approaches to implement steps 2-4 can be taken: Command
 line interface approach and embedded approach.
\end_layout

\begin_layout Subsubsection
Command line approach for the workflow
\end_layout

\begin_layout Standard

\emph on
TODO: write command line wrappers for steps 2-4
\end_layout

\begin_layout Subsubsection
Embedded approach
\end_layout

\begin_layout Standard
See 
\begin_inset CommandInset href
LatexCommand href
name "Example1.java"
target "https://github.com/dlyubimov/HBase-Lattice/blob/dev-0.1.x/sample/src/main/java/Example1.java"

\end_inset

 in sample module for the full cycle of steps 2-6.
 That's actually how we use HBase Lattice: a 100% embedded client application
 that handles scheduling of incremental compilations, schema updates and
 code generation.
\end_layout

\begin_layout Paragraph
Note on how to run Example1.java:
\end_layout

\begin_layout Standard
Compile project with maven so that example job file is built (it will be
 used by Hadoop at the backend).
 Then open example1.java in Eclipse and choose 
\begin_inset Quotes eld
\end_inset

Run As
\begin_inset Quotes erd
\end_inset

 
\begin_inset Formula $\rightarrow$
\end_inset

 
\begin_inset Quotes eld
\end_inset

Java application
\begin_inset Quotes erd
\end_inset

.
 For out-of-Eclipse experience, you need to run it from the 
\begin_inset Quotes eld
\end_inset

sample
\begin_inset Quotes erd
\end_inset

 maven module directory so the hadoop job file is found.
\end_layout

\begin_layout Subsubsection
\begin_inset CommandInset label
LatexCommand label
name "sub:R-approach-to"

\end_inset

R approach to the workflow
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout

{
\backslash
color{tentative}
\end_layout

\end_inset

as of hbl-0.2.0-SNAPSHOT, ecoadapters-0.4.0-SNAPSHOT
\begin_inset ERT
status open

\begin_layout Plain Layout

}
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

{
\backslash
color{tentative}
\end_layout

\end_inset

Starting with hbl-0.2.0, R package 'hblr' is available to perform all workflow
 tasks except for running incremental cube compilation (step 3 and 4 in
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
S
\end_layout

\end_inset

 
\begin_inset CommandInset ref
LatexCommand ref
reference "sub:Overview-of-the"

\end_inset

).
\end_layout

\begin_layout Standard
Package 
\begin_inset Quotes eld
\end_inset

hblr
\begin_inset Quotes erd
\end_inset

 is using R5 reference classes to describe query and HBL admin classes.
\end_layout

\begin_layout Standard
Package 
\begin_inset Quotes eld
\end_inset

hblr
\begin_inset Quotes erd
\end_inset

 requires two packages: 
\begin_inset Quotes eld
\end_inset

rJava
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

ecor
\begin_inset Quotes erd
\end_inset

.
 The latter is a non-standard package from 
\emph on
ecoadapters 
\emph default
project which is the dependency of the HBL.
 If compiled and installed from sources (see 
\emph on
hblr/install.sh
\emph default
 script for an example how to compile and install binary R package from
 sources) then another package, 
\begin_inset Quotes eld
\end_inset

roxygen2
\begin_inset Quotes erd
\end_inset

 and its dependencies are required in order to generate R help files from
 the source annotations.
\end_layout

\begin_layout Paragraph
Deploying/updating cube model from an R script.
\end_layout

\begin_layout Standard
The following is an example of deploying/updating Example1 model using R
 script.
 
\family typewriter
HblAdmin
\emph on
 
\family default
\emph default
R5 class is defining HBL admin functions.
\begin_inset ERT
status open

\begin_layout Plain Layout

}
\end_layout

\end_inset


\end_layout

\begin_layout LyX-Code
# deploy/update HBL cube model from file 	
\end_layout

\begin_layout LyX-Code
hblAdmin <- hbl.HblAdmin$new(model.file.name=
\end_layout

\begin_deeper
\begin_layout LyX-Code
"~/projects/github/hbase-lattice/sample/src/main/resources/example1.yaml")
 	
\end_layout

\end_deeper
\begin_layout LyX-Code
hblAdmin$deployCube() 
\end_layout

\begin_layout Paragraph
Running prepared hbl query from an R script.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

{
\backslash
color{tentative}
\end_layout

\end_inset


\begin_inset Quotes eld
\end_inset

hblr
\begin_inset Quotes erd
\end_inset

 package provides ability to run a prepared hbl query in an R script and
 convert result to a dataframe.
 
\family typewriter
HblQuery
\emph on
 
\family default
\emph default
is the R5 class defining prepared HBL query api.
 The following is an example how to query 
\begin_inset Quotes eld
\end_inset

example1
\begin_inset Quotes erd
\end_inset

 compiled data for particular date/time interval and dimension :
\begin_inset ERT
status open

\begin_layout Plain Layout

}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

<<sampleRQuery, cache=T>>=
\end_layout

\begin_layout Plain Layout

library(hblr) 
\end_layout

\begin_layout Plain Layout

q <- hbl.HblQuery$new(c(
\end_layout

\begin_layout Plain Layout

	"select dim1, COUNT(impCnt) ", 							
\end_layout

\begin_layout Plain Layout

	"as impCnt from Example1 ", 							
\end_layout

\begin_layout Plain Layout

	"where impressionTime in [?,?),", 							
\end_layout

\begin_layout Plain Layout

	"dim1 in [?] ", 							
\end_layout

\begin_layout Plain Layout

	"group by dim1")); 
\end_layout

\begin_layout Plain Layout

timerange <- strptime(c("2011/9/1", "2011/11/1"), 			
\end_layout

\begin_layout Plain Layout

"%Y/%m/%d") 
\end_layout

\begin_layout Plain Layout

q$setParameter(0,timerange[1]) 	
\end_layout

\begin_layout Plain Layout

q$setParameter(1,timerange[2]) 	
\end_layout

\begin_layout Plain Layout

q$setParameter(2,"1") 
\end_layout

\begin_layout Plain Layout

system.time(dframe <- q$execute())  
\end_layout

\begin_layout Plain Layout

dframe 
\end_layout

\begin_layout Plain Layout

@
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Note the use of half-open slice specification for time slices.
 This is a recommended way to slice time series data.
 Since time dimension is not really continuous but is transformed into a
 time hierarchy with lowest granularity of 1 hour, the slice spec of [
\begin_inset Quotes eld
\end_inset

2011/9/1
\begin_inset Quotes erd
\end_inset

, 
\begin_inset Quotes eld
\end_inset

2011/11/1
\begin_inset Quotes erd
\end_inset

] really means range from 
\begin_inset Quotes eld
\end_inset

2011/9/1 00:00:00 UTC
\begin_inset Quotes erd
\end_inset

 to 
\begin_inset Quotes eld
\end_inset

2011/11/1 01:00:00 UTC
\begin_inset Quotes erd
\end_inset

 i.e.
 includes 1 more hour than the obvious intent here.
 To fix the situation, a half-open slice spec 
\begin_inset Quotes eld
\end_inset

[?,?)
\begin_inset Quotes erd
\end_inset

 is used.
\end_layout

\begin_layout Standard
Another example on the test data(lifetime):
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

<<SampleQueryAndPlot,cache=T>>=
\end_layout

\begin_layout Plain Layout

library(ggplot2)
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

dframe <- hbl.HblQuery$new(c(
\end_layout

\begin_layout Plain Layout

		"select dim1, SUM(impCnt) as impCnt, ",
\end_layout

\begin_layout Plain Layout

		"SUM(click) as clickCnt ",
\end_layout

\begin_layout Plain Layout

		"from Example1 group by dim1"))$execute();
\end_layout

\begin_layout Plain Layout

dframe
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

#rearrange
\end_layout

\begin_layout Plain Layout

dframe <- rbind(
\end_layout

\begin_layout Plain Layout

	data.frame(dim1=dframe$dim1,count=dframe$impCnt,group="impressions"),
\end_layout

\begin_layout Plain Layout

	data.frame(dim1=dframe$dim1,count=dframe$clickCnt,group="clicks"))
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

#flipped barplot with dodge by event category
\end_layout

\begin_layout Plain Layout

ggplot(dframe,aes(x=dim1,y=count,fill=group))+
\end_layout

\begin_layout Plain Layout

		geom_bar(stat="identity",position="dodge")+
\end_layout

\begin_layout Plain Layout

		coord_flip()+
\end_layout

\begin_layout Plain Layout

		opts(
\end_layout

\begin_layout Plain Layout

				axis.text.y=theme_text(angle=90,size=8),
\end_layout

\begin_layout Plain Layout

				legend.position="bottom")
\end_layout

\begin_layout Plain Layout

@
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

{
\backslash
color{tentative}
\end_layout

\end_inset

Another test in the R sample code produces time series histogram over entire
 range of simulated data (which in Example1 simulates only few days in each
 month, progressively simulating increased impression log entries day-to-day)
 spanning several months worth of impression counts (
\begin_inset CommandInset ref
LatexCommand formatted
reference "fig:R-histogram-of"

\end_inset

)
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename Example1-R-hist.pdf
	width 90col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:R-histogram-of"

\end_inset

R histogram of impression counts in simulated logs spanning several months
 worth of data simulated by 
\emph on
Example1.java
\emph default
.
 
\begin_inset Newline newline
\end_inset

The binning queries time and plotting time will not depend on actual count
 of impressions happened during that period since the cube already does
 data weighting for us.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
Further and up-to-date details could be found in R help system for the 
\emph on
hblr
\emph default
 package.
\begin_inset ERT
status open

\begin_layout Plain Layout

}
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Specifying a model
\end_layout

\begin_layout Standard
Model is specified by composing a bunch of java classes representing cube,
 cuboids, hierarchies, dimensions and measures.
 Instead of writing some java code wiring this composition up, it is also
 to use a declarative approach for model definition.
 We use YAML for declarative model definition (see file 
\family typewriter

\begin_inset CommandInset href
LatexCommand href
name "example1.yaml"
target "https://github.com/dlyubimov/HBase-Lattice/blob/dev-0.1.x/sample/src/main/resources/example1.yaml"

\end_inset


\family default
 in the 
\emph on
sample
\emph default
 module of the project for an example of a declarative model defintion).
 We employ 
\emph on

\begin_inset CommandInset href
LatexCommand href
name "SnakeYaml"
target "http://code.google.com/p/snakeyaml/"

\end_inset


\emph default
 project for the purposes of constructing java model objects, see how 
\emph on
SnakeYaml
\emph default
 works with classes and constructors for details of what's going on there.
\end_layout

\begin_layout Standard
Once model is defined, it needs to be deployed with HblAdmin (see 
\begin_inset CommandInset href
LatexCommand href
name "Example1.java"
target "https://github.com/dlyubimov/HBase-Lattice/blob/dev-0.1.x/sample/src/main/java/Example1.java"

\end_inset

 in sample module for the example of embedded approach and javadoc for the
 
\family typewriter
HblAdmin
\family default
 class).
\end_layout

\begin_layout Standard

\emph on
TODO: create command line utility wrapping HblAdmin functionality for model
 deployments/updates.
\end_layout

\begin_layout Subsubsection
Supported dimension types
\end_layout

\begin_layout Itemize

\family typewriter
HexDimension
\family default
.
 This class supports discrete dimensions that are fixed-length byte arrays.
 In hbase composite keys they are translated into ASCII Hex representation
 of such for the sake of simpler readability when using tools like hbase
 shell.
 Hence, the name.
\begin_inset Newline newline
\end_inset

Typically, 
\family typewriter
HexDimension
\family default
 is suitable to represent uniformly-distributed hash IDs or otherwise hash-refer
enced data.
 It accepts java type 
\family typewriter
byte[]
\family default
 and its Pig equivalent (in context of compilation).
 Now also accepts any Pig numerical types and converts them to key using
 Big Endian conversion.
 Results returned from queries are always of 
\family typewriter
byte[]
\family default
 type.
\begin_inset Newline newline
\end_inset


\family typewriter
HexDimension
\family default
 is initialized with the value length.
 Slice parameters and fact streams can now also have it as strings encoded
 in hex (equivalent to output of 
\family typewriter
hex() 
\family default
function in 
\emph on
MySQL
\emph default
).
 The fact stream compiler will accept all values shorter or equal in length
 to the one specified in model.
 
\end_layout

\begin_layout Itemize

\family typewriter
SimpleTimeHourHierarchy
\family default
.
 This is a hierarchical dimension to convert 
\family typewriter
GregorianCalendar
\family default
 and/or long values representing ms since epoch in fact streams into hierarchica
l descrete type [ALL].[YEAR-MONTH].[DATE-HOUR].
 I.e.
 the lowest bucket granularity for time series data is 1 hour.
 The continuous member data type for this dimension (when not expressed
 with a hierarchy member) is 
\family typewriter
GregorianCalendar
\family default
, or 
\family typewriter
Long
\family default
 expressing number of milliseconds since epoch (in context of projection
 compilation in pig).
\end_layout

\begin_layout Itemize

\family typewriter
Utf8CharDimension
\family default
.
 This is almost the same as 
\family typewriter
HexDimension 
\family default
but expects string type and encodes it in Utf-8 encoding (meaning collation
 rules of encoded form of Utf8 strings in HBase).
 It can optionally quietly allow truncation if fact length exceeds defined
 storage length (see constructor parameters for details).
 Care should be taken to ensure uniform distribution of queries for this
 dimension if it is used first in a cuboid to scale up hbase query volume.
\end_layout

\begin_layout Standard
Dimension types are assumed to be discrete.
 If dimension is computed over a continuous value in the fact stream (such
 as time), it has to be mapped into a discrete one perhaps by means of using
 a hierarchy (e.g.
 current 
\family typewriter
SimpleTimeHourHierarchy 
\family default
copes with continuous nature of time by setting up discrete member values
 as hour-long buckets and thus implicitly converts every time fact from
 the fact stream into [YEAR-MONTH].[DATE-HOUR] form.
 Obviously, we can make granularity finer and finer so that eventually it
 may go all the way down to a millisecond, and still be able to optimize
 queries with additive and complement scans.
 
\end_layout

\begin_layout Paragraph
Handling NULL values for dimensions in the fact stream.
\end_layout

\begin_layout Standard
At this time, compiler will not accept NULL values for dimensions because
 representation of NULL values is not currently supported while representing
 dimension values as a part of a composite hbase key.
 If it is desirable to accept quietly NULL values as dimensional values
 in fact stream, such NULLs may be transformed into a reserved non-null
 value in the compiler's preamble script.
 Alternatively, some dimension implementations (
\family typewriter
HexDimension
\family default
 and 
\family typewriter
Utf8CharDimension)
\family default
 allow specifying such automatic NULL2non-null substitution as a parameter
 of their constructors.
\end_layout

\begin_layout Subsubsection
Supported measure types
\end_layout

\begin_layout Standard
The measure type per se is currently not limited to a particular type.
 It can be any type.
 However, aggregate functions will only support a particular type.
 If the fact is not of the type an aggregate function would expect during
 compilation, it will be considered NULL without any additional noise.
 
\end_layout

\begin_layout Standard
Most of the aggregate functions like SUM(), COUNT() are supporting facts
 of a numerical type only.
 In the 
\emph on
yaml 
\emph default
model this is defined as 
\family typewriter
NumericMeasure
\family default
.
 
\end_layout

\begin_layout Standard
Currently, compiler also recognizes pig tuples in a form of 
\begin_inset Formula $\left(x,t\right)$
\end_inset

 tuples where x is a numeric fact and 
\begin_inset Formula $t\in\mathbb{N}_{1}$
\end_inset

 long type representing time of the sample.
 Such facts are converted to an instance of 
\family typewriter
IrregularSample
\family default
 class and subsequently are used with exponentially weighted averaging and
 rate functions.
 To define an irregular fact sampling with time datum, 
\family typewriter
IrregularSampleMeasure
\family default
 must be used in the model
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
See example for the usage details.
\end_layout

\end_inset

.
 
\family typewriter
IrregularSampleMeasure 
\family default
type is useful when aggregate function wants to handle time series without
 binning limitations assumed by time dimension hierarchies.
 Still, such function has to adhere to combine and (optionally) complement
 contracts to be able to merge its state.
\end_layout

\begin_layout Paragraph
Handling NULL values for measures in the fact stream.
\end_layout

\begin_layout Standard
Measure values could be NULL in the fact stream.
 It may affect behavior of certain aggregate functions similar to their
 definition in Pig and SQL.
 E.g.
 if slice did not contain non-NULL facts for a measure, SUM() will produce
 NULL but COUNT() will produce 0.
\end_layout

\begin_layout Subsubsection
Specifying cuboids.
\end_layout

\begin_layout Standard
Cuboid specification basically has about the same meaning as choosing aggregate
 tables in ROLAP solutions.
 The main atribute of cuboid specification is a 
\emph on
sequence of dimensions
\emph default
.
 It is a sequence because order is important from both performance point
 of view and range of queries a cuboid may serve.
\end_layout

\begin_layout Standard
The question of what cuboids is fairly complex.
 The general philosophy behind data structures as much the same as in regular
 RDBMS or OLAP.
 Selection of which data structure to build has the same goal: we don't
 know which queries exactly will be used.
 However, we can identify some 
\emph on
families
\emph default
 of queries that may superset the actual query set used, and choose to optimize
 these query supersets.
 E.g.
 a DBA does not exactly know what queries will be run in DBMS, but he or
 she may choose to optimize families of queries thru supporting index range
 scan on a particular attribute of a particular table, thus making decisions
 about particuar index.
\end_layout

\begin_layout Standard
Cuboids are much the same.
 Each cuboid can serve, or is 
\emph on
suitable,
\emph default
 for a particular family of possible queries.
 Further discussion about cuboid suitability can be found in 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
S
\end_layout

\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "sub:Suitable-cuboid"

\end_inset

.
 Different cuboids may have, and in practice often do, an intersection of
 queries they both are suitable for; however, their performace in serving
 particular set of queries may differ as further discussed in 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
S
\end_layout

\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Optimization-and-known"

\end_inset

.
 If there are multiple choices for the same query, usually HBL does the
 right thing and chooses cuboid which in practice is likely to be much faster
 to process.
 So for most part we want to be concrened mostly about cuboid suitability
 rather than performance.
 However, it doesn't mean that we should create just most generally suitable
 cuboids.
 Usually very general cuboids (i.e.
 having most dimensions) are performing well only for a fraction of their
 servicable query sets; and to answer specific queries faster, creation
 of cuboids with very few dimensions is still advised, since they reduce
 amount of scanning and real time aggregation required.
\end_layout

\begin_layout Subsection
Incremental cube compilation
\end_layout

\begin_layout Standard
Cube compilation is done via incremental Pig script dynamically generated
 by compilter component (see sample module for example of the compilation).
 With the current approach, compiler doesn't support any input adapters,
 so it cannot read any standard fact stream sources on its own.
 Instead, it relies on a fragment of the script that reads input into a
 predefined Pig relation, to be supplied.
 This Pig-scripted fragment is called 
\emph on
preamble
\emph default
 and expected to be supplied via Spring 
\family typewriter
Resource
\family default
 specification.
 
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
Perhaps this only dependency on 
\emph on
Spring
\emph default
 is bad and it is worth considering getting rid of this abstraction; but
 developing a project-specific resource abstraction is probably just as
 equally bad.
 Besides, it facilitates wiring compiler bean up using 
\emph on
Spring
\emph default
, which is what we do.
\end_layout

\end_inset

 The compiler expects fact stream to be put in a Pig relation with a predefined
 name (
\begin_inset Quotes eld
\end_inset

HBL_INPUT
\begin_inset Quotes erd
\end_inset

 by default).
 See 
\begin_inset CommandInset href
LatexCommand href
name "Preamble script in sample module"
target "https://github.com/dlyubimov/HBase-Lattice/blob/dev-0.1.x/sample/src/main/resources/example1-preambula.pig"

\end_inset

.
\end_layout

\begin_layout Standard
If the script is being run directly in Pig's executable, preamble perhaps
 should also register hbl jar as additional classpath jar with 
\family typewriter
register()
\family default
.
 
\emph on

\begin_inset Foot
status collapsed

\begin_layout Plain Layout

\emph on
TODO: create maven build for default hadoop job jar for standalone pig applicati
on.
 Example module builds its own hadoop job jar that includes hbl and the
 pig and uses PigContext api to communicate with Grunt mechanics which is
 the way we run it, i.e.
 100% embedded way.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Preamble is also an opportunity to massage fact stream data a little bit
 before handing off to compiler.
 Preamble doesn't suit to host complex preprocessors though because of the
 codegen'd nature of the final script.
 If complex data preprocessing is required, it probably better be done by
 the logging application or a separate preprocessing MR job.
\end_layout

\begin_layout Subsubsection
\begin_inset CommandInset label
LatexCommand label
name "sub:Fact-stream-Pig"

\end_inset

Fact stream Pig schema requirements
\end_layout

\begin_layout Standard
The requirements for HBL_INPUT relation must have a 
\emph on
Pig schema
\emph default
 and such schema must satisfy the following: 
\end_layout

\begin_layout Itemize
It must have all defined dimensions.
 Dimension names used must be the same as in model description.
 Dimension Pig types depend on the dimension class.
\end_layout

\begin_layout Itemize
It must have at least one measure fact.
 The measure is recognized by having the same name as in model description.
 The scope of measures may be reduced by using exclude/include api on the
 compiler bean (see sample module for an example).
 By default, all measures are expected.
 Using measure scope reduction allows to easily compile in multiple fact
 streams containing different measures and potentially originating in different
 sources (for as long as all dimensions can be inferred for each of them).
 
\end_layout

\begin_layout Itemize
Fact stream items should have Pig types supported by their respective dimension
 and measure types.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

{
\backslash
color{comment}
\end_layout

\end_inset

TODO: command line utility to run compiler bean to generate the pig script
 into a file.
 Perhaps same for R hblr package.
\begin_inset ERT
status open

\begin_layout Plain Layout

}
\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Multiple fact streams and compilation process partitioning
\end_layout

\begin_layout Standard
In HBL there are two ways to partition process of adding new facts to the
 cube: by measures and by cuboids.
\end_layout

\begin_layout Paragraph
Partitioning compilation process by measures.
\end_layout

\begin_layout Standard
There's often a requirement to process not one stream but rather several
 streams.
 A typical example (from the world of ad networks) is when the system collects
 impression logs and click logs as separate fact datasets.
 Suppose we want to have 2 measures in the cube, impression count and click
 count.
 Then the processing of the impression log may be transformed to contain
 all dimensions (say, time, domain id or hash) and only single numeric measure
 -- impression count.
 On the other hand, click log may have all dimensions as well and a click
 count.
 Processing these two fact streams disjointly and asynchronously will effectivel
y build the complete cube with both measures.
\end_layout

\begin_layout Standard
Under these circumstances, we want to set up two independent compiler cycles
 (and thus, two generated pig scripts).
 When generating compiler script for impressions, we want to specify to
 process impression counts only.
 We can select a particular subset of measures by specifying 
\family typewriter
Pig8CubeIncrementalCompiler
\begin_inset ERT
status open

\begin_layout Plain Layout

\family typewriter

\backslash
-
\end_layout

\end_inset

Bean
\begin_inset ERT
status open

\begin_layout Plain Layout

\family typewriter

\backslash
-
\end_layout

\end_inset

#set
\begin_inset ERT
status open

\begin_layout Plain Layout

\family typewriter

\backslash
-
\end_layout

\end_inset

MeasureInclude()
\family default
 and 
\family typewriter
Pig8CubeIncrementalCompilerBean#setMeasureExclude()
\family default
.
 This technique has been briefly mentioned in 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
S
\end_layout

\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "sub:Fact-stream-Pig"

\end_inset

.
 Symmetrically similar considerations apply when compiling click facts.
\end_layout

\begin_layout Paragraph
Partitioning compilation process by cuboids.
\end_layout

\begin_layout Standard
Sometimes it makes sense to skip certain cuboid compilations when compiling
 based on a certain stream.
 Suppose, as in the previous example, we have impression and click streams.
 Suppose, impression stream has knowledge of impression time dimension but
 click stream has both impression time by that time, and the click time
 as well.
 Since click time is not really available during processing the impression
 facts, it doesn't make sense to compile any cuboids containing click time
 hierarchy during impression log compilation since this doesn't create any
 interesting knowledge but would decrease (sometimes, quite significantly)
 the compiler load.
\end_layout

\begin_layout Standard
HBL handles cuboid inclusion/exclusion by means of assigning a 
\emph on
compilerGroup
\emph default
 to each cuboid specification (which is optional) and then using 
\family typewriter
Pig8CubeIncrementalCompilerBean#setCuboidGroupsInclude()
\family default
 to specify cuboid groups to be included in that particular compilation
 cycle.
 (By default, all cuboids are compiled).
 Seel also 
\family typewriter

\begin_inset CommandInset href
LatexCommand href
name "example1.yaml"
target "https://github.com/dlyubimov/HBase-Lattice/blob/dev-0.1.x/sample/src/main/resources/example1.yaml"

\end_inset


\family default
 for an example of 
\emph on
compilerGroup
\emph default
 attribute use.
\end_layout

\begin_layout Subsection
Deploying HBL custom HBase filters
\end_layout

\begin_layout Standard
Since custom filters are used for querying, one jar (hbl-0.1.2.jar as of the
 time of this writing) should be deployed to region servers (perhaps with
 a rolling restart afterwards).
 With CDH distribution it turns out it is enought just to drop hbl.jar into
 $HBASE_HOME/lib folder at the region servers.
 Only querying part depends on this, incremental compiler does not depend
 on this.
\end_layout

\begin_layout Subsection
Querying
\end_layout

\begin_layout Standard
Within the Java world, there are 3 different ways to query HBL: 
\end_layout

\begin_layout Itemize

\family typewriter
AggregateQuery 
\family default
which is a declarative way to add slicing and grouping predicates (not unlike
 Hibernate's Query).
\end_layout

\begin_layout Itemize

\family typewriter
PreparedAggregateQuery: 
\family default
the HBL query language to add slicing and grouping predicates, as well as
 inline application of aggregate functions.
 This is quite similar to JDBC's PreparedQuery (and perhaps there's not
 much rationale why JDBC could not be eventually used except JDBC is much
 more demanding in terms of metadata).
\end_layout

\begin_layout Itemize

\family typewriter
HblInputFormat 
\family default
which is a way to run HBL query in a locality-sensitive distributed way
 as well as continue working on HBL results in a MapReduce environment.
\end_layout

\begin_layout Standard
There's also an R package available to run HBL Queries and transform the
 results into an R data frames, which is basically a wrapper around 
\family typewriter
PreparedAggregateQuery
\family default
 API along with some data transfer mechanism from Java to R.
 This is described in brief detail in 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
S
\end_layout

\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "sub:R-approach-to"

\end_inset

.
\end_layout

\begin_layout Subsubsection
Declarative predicates (
\begin_inset Formula $\propto$
\end_inset


\family typewriter
AggregateQuery
\family default
 )
\end_layout

\begin_layout Standard
See Example1.java for examples of uses of 
\family typewriter
AggregateQuery.

\family default
 Java query api is used to specify slicinig and grouping predicates (not
 unlike in Hibernate's Query API).
\end_layout

\begin_layout Subsubsection
Querying with a prepared query 
\begin_inset Newline newline
\end_inset

(
\begin_inset Formula $\propto$
\end_inset


\family typewriter
PreparedAggregateQuery
\family default
)
\end_layout

\begin_layout Standard

\emph on
TODO: write a command line shell (perhaps akin to HBase shell) to enable
 ad-hoc query runs.
\end_layout

\begin_layout Standard
See the example for how to prepare and use query.
 It is recommended to use prepared query repeatedly to save on parsing it
 into an AST tree.
 (After all, that's what prepared queries are for).
\end_layout

\begin_layout Standard
Approximate current query syntax is (see RFC-822 for the BNF syntax used):
 
\end_layout

\begin_layout Standard

\family typewriter
\begin_inset Box Boxed
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "95col%"
special "none"
height "1in"
height_special "totalheight"
status collapsed

\begin_layout Plain Layout

\family typewriter
'select' select-expr *(',' select-expr) 'from' cube-name [where-clause]
 [group-clause]
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Plain Layout

\family typewriter
select-expr = measure-name / aggr-function [ 'as' alias-name ]
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Plain Layout

\family typewriter
aggregate-function = function-name '(' measure-name ')'
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Plain Layout

\family typewriter
where-clause = 'where' slice-spec *(',' slice-spec)
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Plain Layout

\family typewriter
slice-spec = dimension-name 'in' ('[' / '(') value / '?' [ ',' ( value /
 '?' ) ] (']' / ')')
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Plain Layout

\family typewriter
group-clause = 'group by' dimension-name *(, dimension-name)
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Plain Layout

\family typewriter
measure-name = ID / '?' ; id rules or substitution via a parameter
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Plain Layout

\family typewriter
cube-name = ID / '?' ; id rules or substitution via a parameter
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Plain Layout

\family typewriter
alias-name = ID / '?' ; id rules or substitution via a parameter
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Plain Layout

\family typewriter
function-name = ID / '?' ; id rules or substitution via a parameter
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Plain Layout

\family typewriter
dimension-name = ID / '?' ; id rules or substitution via a parameter
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Plain Layout

\family typewriter
value = ( '
\backslash
'' LITERAL '
\backslash
'' ) / LONG / DOUBLE
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Example:
\end_layout

\begin_layout Standard
\begin_inset Box Boxed
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "95col%"
special "none"
height "1in"
height_special "totalheight"
status open

\begin_layout Plain Layout

\family typewriter
select d1 as dim1, COUNT( m1 ) from Example where d1 in [?], time in [?,?)
 group by d1
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\emph on
Where-clause 
\emph default
is essentially a slice specification.
 Hence specification is imposed on a dimension using opened or closed interval
 semantics.
 E.g.
 [1,3) is a so-called half-open interval which includes between values of
 1 (including) and 3 (excluding).
 The limitation of the 
\emph on
where-clause 
\emph default
is that currently one cannot specify more than one slice specification for
 the same dimension.
 Semantic result of an attempt to specify multiple slices for the same dimension
 is currently undefined.
 
\end_layout

\begin_layout Standard
Aggregating over multidimensional hyperplane (a plane perpendicular to an
 axis and going thru a specific point on that axis) is hence equivalent
 to specifying '
\family typewriter
where dimension in [?]
\family default
' (
\emph on
degenerate 
\emph default
dimension interval).
\end_layout

\begin_layout Standard
Aggregate functions may return 
\family typewriter
NULL
\family default
 if a measure group had been empty (or consisted only of 
\family typewriter
NULL
\family default
 measure values).
 This semantics is consistent with 
\family typewriter
SUM()
\family default
 and some other aggregate functions semantics in SQL and Pig.
 As a corner case, a measure group might have been empty if reduced measure
 scope was applied during compilation.
 Reduced scope fact stream basically is equivalent to a full fact stream
 having all facts for the excluded measures as 
\family typewriter
NULL
\family default
.
\end_layout

\begin_layout Paragraph
Query limitations.
\end_layout

\begin_layout Itemize
There has to be a suitable cuboid present and built (see 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
S
\end_layout

\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "sub:Suitable-cuboid"

\end_inset

.)
\end_layout

\begin_layout Itemize
Complement scan optimizations for hierarchies are not implemented in this
 release (only in our prototype).
\end_layout

\begin_layout Itemize
There's currently no way to run some useful analytic queries like 'select
 COUNT(fact), ip group by ip having COUNT(fact) > 10000'.
 
\end_layout

\begin_layout Itemize
One has to select at least one measure aggregate in the query.
 Technically, there should be no reason why not support a request for dimension
 members satisfying 
\emph on
where-clause
\emph default
 conditions only, but the way it is currently designed, it need a least
 one measure to sum up in an aggregate (even if one doesn't use it).
\end_layout

\begin_layout Itemize
Unlike with MDX, there's no 
\emph on
optimized 
\emph default
way to query a dimension or hierarchy membership.
 Since the system is pretty dynamic, new members might appear at any time
 and at this point we don't keep track of distinct list of them.
 
\emph on
It is possible to query members in a particular slice though, including
 the total cube, 
\emph default
but that would be a full table scan over shortest cuboid still.
\end_layout

\begin_layout Subsubsection
Running location-sensitive query as a MapReduce job 
\begin_inset Newline newline
\end_inset

(
\family typewriter

\begin_inset Formula $\propto$
\end_inset

HblInputFormat
\family default
)
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

{
\backslash
color{tentative}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
One of the most common goals of having multidimensional database is to have
 an exploratory OLAP capability.
 However, sometimes it is useful to run a bulk query and dump certain projection
 of the cube (or work on it) as a batch.
 For example, to dump average performance metrics for all accounts in the
 system for the last month.
 Such query may produce a significant amount of work both on HBase-Lattice
 side and client side that works with the results and thus may benefit enormousl
y from a locality-sensitive worker distribution.
 Since HBase-Lattice queries consist of HBase scans, they can be defined
 as a MapReduce job that tries to optimize scan splits so that results of
 multidimensional query are compiled from HBase region data located on the
 same node as a MapReduce worker task.
\end_layout

\begin_layout Standard
HBase-Lattice project has now support for that in a form of 
\family typewriter
HblInputFormat
\family default
.
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout

{
\backslash
color{comment}
\end_layout

\end_inset

Pig's 
\emph on
LoadFunc
\emph default
 work is still underway at this moment.
\begin_inset ERT
status open

\begin_layout Plain Layout

}
\end_layout

\end_inset


\end_layout

\end_inset

 See 
\family typewriter
MRExample1Query
\family default
 for an example how to run a simple distributed HBL prepared query.
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout

{
\backslash
color{comment}
\end_layout

\end_inset

right now limitation for prepared parameters is that they are serialized
 as string values only thru MR configuration so substitution of parameters
 is limited to cases where string values are supported by the substitution
 use case.
 In particular, I think the date/hour hierarchy currently may be somewhat
 a problem.
\begin_inset ERT
status open

\begin_layout Plain Layout

}
\end_layout

\end_inset


\end_layout

\end_inset

 The total result of the query is equivalent tpo concatenating results of
 all map tasks.
 Extend 
\family typewriter
HblMapper
\family default
 abstract mapper for the mapper task.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

}
\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Querying HBL from R
\end_layout

\begin_layout Standard
This has been outlined as a part of 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
S
\end_layout

\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "sub:R-approach-to"

\end_inset

.
\end_layout

\begin_layout Subsubsection
Supported standard aggregate functions
\end_layout

\begin_layout Itemize
SUM().
 This function returns 
\family typewriter
Double
\family default
 in queries.
 
\begin_inset ERT
status open

\begin_layout Plain Layout

{
\backslash
color{tentative}
\end_layout

\end_inset

As per conventions in Pig and SQL, this function will return NULL for empty
 groups (groups with no single fact encountered).
\begin_inset ERT
status open

\begin_layout Plain Layout

}
\end_layout

\end_inset

 Formally: 
\begin_inset Formula $\mbox{sum}\left(X\right)=\sum_{i=1}^{N}x_{i}$
\end_inset

.
\end_layout

\begin_layout Itemize
COUNT().
 This function returns 
\family typewriter
Long
\family default
 in queries.
 
\begin_inset ERT
status open

\begin_layout Plain Layout

{
\backslash
color{tentative}
\end_layout

\end_inset

 For empty groups, returns 0L.
\begin_inset ERT
status open

\begin_layout Plain Layout

}
\end_layout

\end_inset

 Formaly: 
\begin_inset Formula $\mbox{count}\left(X\right)=N$
\end_inset

.
\end_layout

\begin_layout Itemize
Exponentially (or, rather, Canny function) weighted average for facts with
 time-based irregular sampling 
\series bold

\begin_inset Formula $\left(x,t\right)$
\end_inset

 
\series default
as a custom function in the model.
 This function returns instance of 
\family typewriter
OnlineCannyAvgSummarizer
\family default
 containing the weighted sum state.
 It additionally can be used to derive a biased binomial estimate on a slice
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
see metrics doc and example.
\end_layout

\end_inset

.
\end_layout

\begin_layout Itemize
Exponentially (or, rather, Canny function) weighted rate for facts with
 time-based irregular sampling 
\begin_inset Formula $\left(\mbox{count},t\right)$
\end_inset

 as a custom function (see Example and doc)
\end_layout

\begin_layout Itemize
SUM_SQ() sum of squares: 
\begin_inset Formula $\mbox{sumsq}\left(X\right)=\sum_{i=1}^{N}x_{i}^{2}$
\end_inset


\end_layout

\begin_layout Itemize
AVG() mean: 
\begin_inset Formula $\mu=\frac{1}{N}\sum_{i=1}^{N}x_{i}$
\end_inset


\end_layout

\begin_layout Itemize
SD() standard deviation: 
\begin_inset Formula $\sigma=\sqrt{\frac{1}{N}\sum_{i=1}^{N}\left(x_{i}-\mu\right)^{2}}$
\end_inset

.
 
\begin_inset ERT
status open

\begin_layout Plain Layout

{
\backslash
color{comment}
\end_layout

\end_inset

(todo: add sample deviation with Bessel correction; but see variance for
 a workaround formula).
 
\begin_inset ERT
status open

\begin_layout Plain Layout

}
\end_layout

\end_inset


\end_layout

\begin_layout Itemize
VAR() standard variance: 
\begin_inset Formula $\mbox{var}\left(X\right)=\frac{1}{N}\sum_{i=1}^{N}\left(x_{i}-\mu\right)^{2}$
\end_inset

.
 
\begin_inset ERT
status open

\begin_layout Plain Layout

{
\backslash
color{comment}
\end_layout

\end_inset

(todo: add sample variance with Bessel correction, but as workaround, Bessel
 correction can be obtained now by client performing 
\begin_inset Formula $N\leftarrow\mbox{count}\left(X\right)$
\end_inset

; 
\begin_inset Formula $\mbox{var}_{B}\left(X\right)=\frac{N}{N-1}\cdot\mbox{var}\left(X\right)$
\end_inset

.)
\begin_inset ERT
status open

\begin_layout Plain Layout

}
\end_layout

\end_inset


\end_layout

\begin_layout Itemize
MIN() 
\begin_inset ERT
status open

\begin_layout Plain Layout

{
\backslash
color{tentative}
\end_layout

\end_inset

 Returns NULL for empty groups.
\begin_inset ERT
status open

\begin_layout Plain Layout

}
\end_layout

\end_inset

 Formally: 
\begin_inset Formula $f\left(X\right)=\underset{i\in\left[1,N\right]}{\min}\left(x_{i}\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
MAX() 
\begin_inset ERT
status open

\begin_layout Plain Layout

{
\backslash
color{tentative}
\end_layout

\end_inset

 Returns NULL for empty groups.
 
\begin_inset ERT
status open

\begin_layout Plain Layout

}
\end_layout

\end_inset

 Formally: 
\begin_inset Formula $f\left(X\right)=\underset{i\in\left[1,N\right]}{\max}\left(x_{i}\right)$
\end_inset


\end_layout

\begin_layout Subsubsection
User defined aggregate functions
\end_layout

\begin_layout Standard
Api allows to add new custom aggregate functions fairly easily, it's just
 all we needed at the moment.
 
\begin_inset ERT
status open

\begin_layout Plain Layout

{
\backslash
color{comment}
\end_layout

\end_inset

TODO
\begin_inset ERT
status open

\begin_layout Plain Layout

}
\end_layout

\end_inset


\end_layout

\begin_layout Section
Complement and additive scan query optimizations.
 
\end_layout

\begin_layout Standard
Scanning a point slice of either dimension or hierarchy is trivial.
\end_layout

\begin_layout Standard
Scanning a range of slices of a dimension is trivial as well (assuming that's
 the last dimension in cuboid spec).
\end_layout

\begin_layout Standard
Scanning a range over a hierarchy is less trivial.
 Hierarchy must support notion of 
\family typewriter
[ALL]
\family default
 member aggregates to be able to produce batch.
 Additionally, hierarchy needs to support optimizing for complement vs.
 union scans.
 (time hierarchies come to mind as a particularly good example of benefiting
 from complement scans).
\end_layout

\begin_layout Standard
Here I'll develop a very simple bit of theory behind additive and complement
 scans.
 
\end_layout

\begin_layout Paragraph
Definition - additive scan-capable aggregate functions.
\end_layout

\begin_layout Standard
Suppose we have a bunch of metrics (facts) 
\begin_inset Formula $\mathbf{M}=\left\{ m_{1},m_{2},...,m_{n}\right\} $
\end_inset

.
 We also consider and aggregate function defined over a fact set, 
\begin_inset Formula $\mbox{aggr}\left(\mathbf{M}\right)$
\end_inset

, which returns a single variable.
 If for any two disjoint subsets 
\begin_inset Formula $\mathbf{M}_{1}$
\end_inset

 and 
\begin_inset Formula $\mathbf{M}_{2}$
\end_inset

:
\begin_inset Formula $\mathbf{M}_{1}\cap\mathbf{M}_{2}=\emptyset$
\end_inset

 also satisfying 
\begin_inset Formula $\mathbf{M}_{1}\cup\mathbf{M}_{2}=\mathbf{M}$
\end_inset

 exists a function 
\begin_inset Formula $\mathbf{add}\left(r_{1},r_{2}\right)$
\end_inset

 such that 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\mbox{aggr}\left(\mathbf{M}\right)=\mbox{add}\left(\mbox{aggr}\left(\mathbf{M}_{1}\right),\mbox{aggr}\left(\mathbf{M}_{2}\right)\right),
\]

\end_inset


\end_layout

\begin_layout Standard
then we call function 
\begin_inset Formula $\mbox{aggr}\left(\cdot\right)$
\end_inset

 
\emph on
additive scan-capable.
\end_layout

\begin_layout Paragraph
Definition - complement scan-capable aggregate functions.
\end_layout

\begin_layout Standard
Similarly, every request can be devised into summing scan over metrict fact
 set 
\begin_inset Formula $\mathbf{M}_{1}$
\end_inset

 and a complement scan over another dataset 
\begin_inset Formula $\mathbf{M}_{2}:\mathbf{M}_{2}\subseteq\mathbf{M}_{1}$
\end_inset

.
 
\end_layout

\begin_layout Standard
Suppose there's an existing aggregating function over metric set 
\begin_inset Formula $\mathrm{aggr}\left(\cdot\right)$
\end_inset

 If there exists a function of two variables 
\begin_inset Formula $\mathbf{complement}\left(r_{1},r_{2}\right)$
\end_inset

 such that for any two fact sets 
\series bold

\begin_inset Formula $\mathbf{M}_{1}$
\end_inset

 
\series default
and 
\begin_inset Formula $\mathbf{M}_{2}$
\end_inset

 satisfying 
\begin_inset Formula $\mathbf{M}_{2}\subseteq\mathbf{M}_{1}$
\end_inset

 the following is true 
\begin_inset Formula 
\begin{eqnarray*}
r & = & \mathrm{aggr}\left(\mathbf{M}_{1}\backslash\mathbf{M}_{2}\right)\\
 & = & \mathrm{\mathbf{complement}}\left(\mathrm{aggr}\left(\mathbf{M}_{1}\right),\mathrm{aggr}\left(\mathbf{M}_{2}\right)\right),
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
then we say that 
\begin_inset Formula $\mathrm{aggr}\left(\cdot\right)$
\end_inset

 is 
\emph on
complement scan-capable.
\end_layout

\begin_layout Standard
Obviously, sum(), count() and avg() could be represented in a way that makes
 them complement scan-capable.
\end_layout

\begin_layout Standard
Complement optimization specifically comes in light for time series scans
 that involve timezone corrections, e.g.
 timezone correction for a month hierarchy with additional complement scan
 on an hour hierarchy.
\end_layout

\begin_layout Standard
Hence, it follows that we should model a hierarchy in a way so that its
 implementation would be able to opimize using complement scans.
 We will require scan additivity of all aggregate functions, but it seems
 that we cannot request complement scan capability of any given aggregate
 function.
 Hence, future complement scan optmization should interrogate functions
 as to whether complement scan optimization is possible.
 
\end_layout

\begin_layout Section
Optimization and known deficiencies to watch for
\begin_inset CommandInset label
LatexCommand label
name "sec:Optimization-and-known"

\end_inset


\end_layout

\begin_layout Standard
Most, if not all, of HBL optimization actually revolves around choosing
 the right set of available cuboids.
 Choice of cuboids is encompassed by a set of frequently-run (or potentially
 run) queries.
 This and other optimization/known deficiency topics are discussed here.
\end_layout

\begin_layout Subsection
Suitable cuboid
\begin_inset CommandInset label
LatexCommand label
name "sub:Suitable-cuboid"

\end_inset


\end_layout

\begin_layout Standard
If a query slices by set of dimensions 
\begin_inset Formula $S$
\end_inset

 and groups by set of dimensions 
\begin_inset Formula $G$
\end_inset

 then a suitable cuboid with dimensions 
\begin_inset Formula $C$
\end_inset

 is such that 
\begin_inset Formula $S\cup G\in C$
\end_inset

 
\bar under
and
\bar default
 
\begin_inset Formula $G\cup S_{d}'$
\end_inset

 dimensions are in the lead positions of 
\begin_inset Formula $C$
\end_inset

 (in any order).
 Here, 
\begin_inset Formula $S_{d}\subset S$
\end_inset

 is subset of slicing dimensions where degenerate slicing interval is used
 ( i.e.
 closed degenerate interval such as 
\begin_inset Formula $\left[a,a\right]$
\end_inset

); and 
\begin_inset Formula $S_{d}'\subset S_{d}$
\end_inset

 is any subset of 
\begin_inset Formula $S_{d}$
\end_inset

.
\end_layout

\begin_layout Standard
For example, if we have a query:
\end_layout

\begin_layout Standard

\family typewriter
select A, COUNT(m1) as m1Count from Cube 
\begin_inset Newline newline
\end_inset

where B in [?] group by A
\end_layout

\begin_layout Standard
then any of cuboids with dimensions (A,B); (B,A); (A,C,B) or (B,A,C) etc.
 are suitable to run the query (albeit they will provide different performance;
 optimizer will try to choose an 
\begin_inset Quotes eld
\end_inset

optimal
\begin_inset Quotes erd
\end_inset

 one).
 Cuboids (B,C,A); (C,A,B) or (C,B,A) will not be 
\begin_inset Quotes eld
\end_inset

suitable
\begin_inset Quotes erd
\end_inset

.
\end_layout

\begin_layout Standard
For a similar query:
\end_layout

\begin_layout Standard

\family typewriter
select A, COUNT(m1) as m1Count from Cube 
\begin_inset Newline newline
\end_inset

where B in [?], C in [?,?) group by A
\end_layout

\begin_layout Standard
examples of 
\begin_inset Quotes eld
\end_inset

suitable
\begin_inset Quotes erd
\end_inset

 cuboids are (A,C,B); (A,B,C); (B,A,C); (B,A,D,C) or (A,D,C,B) etc.
 Examples of non-suitable cuboids are (B,C,A); (C,A,B); (C,B,A) etc.
\end_layout

\begin_layout Standard
Currently, such seemingly elaborate criteria of 
\begin_inset Quotes eld
\end_inset

suitability
\begin_inset Quotes erd
\end_inset

 stems from a requirement to produce query results 
\begin_inset Quotes eld
\end_inset

inline
\begin_inset Quotes erd
\end_inset

 with a set of scan merges.
 In-memory result accumulation and re-sorting is prohibited.
 On-disk 
\begin_inset Quotes eld
\end_inset

spills
\begin_inset Quotes erd
\end_inset

 of the results are also being avoided.
 This ensures fastest response and minimal memory requiremnts on the client
 side.
 On the other hand, we want to capture all possibly usable cases, which
 also complicates suitability critera definition a little bit.
\end_layout

\begin_layout Subsection
Queries over shorter cuboids normally run faster
\end_layout

\begin_layout Standard
If an often-run query groups by A and B, it can be served by any of cuboids
 with dimensions (A,B), (A,B,C), (A,B,C,D) but will ran fastest with (A,B).
 Optimizer will make the right choice but only if (A,B) is actually available.
\end_layout

\begin_layout Subsection
Slicing vs.
 grouping 
\end_layout

\begin_layout Standard
If an often-run query groups by A and B and slices by A, it can be served
 by both cuboids with dimensions of (A, B,...) and (B, A, ...) but the former
 will run faster.
 
\begin_inset ERT
status open

\begin_layout Plain Layout

{
\backslash
color{comment}
\end_layout

\end_inset

Currently optimizer does not support discerning between those cases (mostly
 because it is unusual to have both) so it is recommended to make only one
 of those choices available.
\begin_inset ERT
status open

\begin_layout Plain Layout

}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset space ~
\end_inset


\end_layout

\begin_layout Standard
Example 1.
 
\end_layout

\begin_layout LyX-Code

\family typewriter
select A,B, SUM(M) from cube where B in [?] group by A,B
\end_layout

\begin_layout Standard
is best optimized by a presence of cuboid with dimensions (B,A) (unless
 B is a non-uniform key like time).
\end_layout

\begin_layout Standard
\begin_inset space ~
\end_inset


\end_layout

\begin_layout Standard
Example 2.
\end_layout

\begin_layout LyX-Code

\family typewriter
select A,B, SUM(M) from cube where C in [?,?] group by A,B
\end_layout

\begin_layout Standard
is best optimized by a presence of one of cuboids with dimensions (A,B,C)
 or (B,A,C).
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout

{
\backslash
color{comment}
\end_layout

\end_inset

Actually, in case where slicing by C is using a degenerate interval 
\begin_inset Formula $\left[a,a\right]$
\end_inset

 then the best execution plan would perhaps arguably be over a cuboid (C,A,B)
 or (C,B,A).
 Pushing dimension slicing over degenerate interval up the cuboid key chain
 over grouping dimension is now supported as of most recent 0.2.x release
 tag.
\begin_inset ERT
status open

\begin_layout Plain Layout

}
\end_layout

\end_inset


\end_layout

\end_inset

 Further considerations include whether A or B more uniformly distributed
 to cater to better query load distribution among region servers (but not
 running time per se) -- see below.
\end_layout

\begin_layout Subsection
Uniformity of key distribution
\end_layout

\begin_layout Standard
Per HBase tactics, it is important to maintain uniform key distribution
 (at least within certain range) in order to provide uniform query distribution
 as well.
 In hbl, cuboid dimensions are used as cuboid composite keys.
 Therefore, it is important to have a dimension with more or less uniform
 member distribution in a certain range in the lead position of a cuboid
 (typically, a hash or random key value).
 By the same logic, it is almost never useful to put time related hierarchies
 into the lead position unless the nature of application provides uniform
 interest over the entire time period analyzed.
\end_layout

\begin_layout Standard
Additional instruments include traditional HBase techniques such as query
 hot spot detection and elimination (perhaps by inducing a forced split
 on a cuboid table).
\end_layout

\begin_layout Subsection
Long dimension values
\end_layout

\begin_layout Standard
Per HBase schema design, it is quite deficient to use big keys and small
 values because the row and column names are stored (and even communicated
 over the wire) within each cell.
 In HB-L, a cell content is a measure with a number of aggregated states
 of such a measure and the key is the dimensional slice for which the aggregates
 are precompiled.
 Hence, this HBase imposed deficiency (or perhaps 
\begin_inset Quotes eld
\end_inset

feature
\begin_inset Quotes erd
\end_inset

, it's not quite clear to me at this point), translates into deficiencies
 when long dimensional values are used with HBase-Lattice.
 To add insult to injury, HBL dimensional values are stored as a zero-padded
 fixed length type (similarly to char(N) type in databases) to ensure proper
 composite key parsing and collation rules for efficient custom filtering.
 
\end_layout

\begin_layout Standard
That means that if dimesion type is declared to have a length of 3000, the
 same amount of bytes is used to store the dimension value in a cuboid record
 (before compression) and it is also multiplied by number of cells (i.e.
 measures).
 Also, hierarchies generate one distinct key per level (e.g.
 time-hour hierarchy generates one lifetime key, one mohthly key, and one
 hour-of-day key) so several rows may be generated per just a row of input.
 This makes the case of long dimension values quite bad in a geometric proportio
n.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

{
\backslash
color{tentative}
\end_layout

\end_inset

There currently no remedy for this in HBase-Lattice.
 A workaround is to use hashes instead of values for long documents that
 are desired to be used as a dimension.
 The implication of this is also lack of practically useful collate semantics
 of such values for the purposes of querying by interval other than a degenerate
 interval ([?]) since the byte-order hbase collation is imposed over hashes
 rather than actual dimension values.
 Current thought is to incorporate that workaround into HBL compiler by
 means of mantaining membership tables and dimension member properties and
 using so-called 'blob' dimensions.
 However, resolving hashes into actual values yanked from a member table
 would result in one additional 
\emph on
get 
\emph default
per each returned blob key, imposing additional latency overhead.
 To some degree, this can be somewhat helped by MultiGet facitlity.
 
\begin_inset ERT
status open

\begin_layout Plain Layout

}
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Dimensions with large membership count
\end_layout

\begin_layout Standard
If the membership count is large and incremental cycles are short, it may
 turn out that incremental data chunks aggregate poorly accross cuboids
 and you may find that compilation generates hbase traffic that is proportional
 to the number of facts.
 The sign of this is 
\begin_inset Quotes eld
\end_inset

slow
\begin_inset Quotes erd
\end_inset

 running reducers of the first MR job of the compiler cycle.
 
\end_layout

\begin_layout Standard
Adding more reducer tasks in this case will not help much because the bottleneck
 is hbase traffic i.e.
 I/O from reducer's point of view, and it probably will make things worse
 by creating a possiblity of overloading HBase in some cases.
 My educated and anecdotally verified guess about # of reducers is about
 120% of the total number of region servers in the system, provided lead
 cuboid keys are uniformly distributed.
 
\end_layout

\begin_layout Standard
The remedy to that is one of 
\end_layout

\begin_layout Itemize
consider using better aggregating cuboids, or 
\end_layout

\begin_layout Itemize
increase the period of compilation so the data aggregates perhaps better,
 or 
\end_layout

\begin_layout Itemize
scale out HBase itself to be able to handle increased query traffic (combining
 existing counters with the added group counts), or
\end_layout

\begin_layout Itemize
a combination of the above.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

{
\backslash
color{tentative}
\end_layout

\end_inset

It is also possible to split same cube incremental compilation cycle into
 several cycles with different periods.
 I.e.
 it is possible to compile cuboids that aggregate better with more frequency
 than those that require to accumulate more data (i.e.
 time) to aggregate better, in several distinct compiler runs using compiler
 groups.
 The data will be 
\begin_inset Quotes eld
\end_inset

eventually
\begin_inset Quotes erd
\end_inset

 consistent among cuboids, although it would take different time to get
 results into cube.
 Another thing to think about in this case is to use coarser time hierarchies
 for cuboids that compile less often (unless finer granularity is still
 important even that availability exceeds it by a respectful margin).
 
\begin_inset ERT
status open

\begin_layout Plain Layout

}
\end_layout

\end_inset


\end_layout

\begin_layout Section
TODOs and FIXME
\end_layout

\begin_layout Subsection
Assorted issues
\end_layout

\begin_layout Standard
At this point there's no JDBC provider available (we don't use jdbc; we
 integrate custom datasources directly into our reporting tool.
 Therefore, creating jdbc support ranked very low on our roadmap, but assuming
 there's an external interest in this, it should be an easy enhancement,
 all components are already there for it).
\end_layout

\begin_layout Standard

\series bold
Complement scan optimizations for hierarchies are not in yet
\series default
.
 (but there's a working prototype).
 
\end_layout

\begin_layout Standard
Access to model elements is rudimental.
 Model is exchanged as yaml-serialized string within compiler, which may
 overload pig communication to backend wastefully if model becomes sizeable
 (thousands of measures or dimensions).
 System tables containing model is rudimental as well.
 We seem to employ several dozens of measures in production without noticeable
 problems, but obviously this is a somewhat severe limitation for a 
\begin_inset Quotes eld
\end_inset

big data
\begin_inset Quotes erd
\end_inset

 system.
\end_layout

\begin_layout Standard
Poor selection of aggregate functions.
 Modelling for aggregation functions and supported member types needs more
 thought, it is not flexible enough right now.
\end_layout

\begin_layout Standard
Poor selection of hierarchy and dimension types.
 Add more fine grained time hierarchy.
 Queries don't support specifying hierarchy members in their hierarchical
 inline syntas as in [ALL][2011][JAN] , but only as a continuous value.
 Hierarchical members can be constructed and passed in as parameters though.
\end_layout

\begin_layout Standard
Crosstab output is not formatted as tab (although equivalent data can be
 returned as adjacency list).
\end_layout

\begin_layout Standard
Poor selection of measure types.
 Currently, we are limited to numeric facts in fact stream only.
\end_layout

\begin_layout Standard
Support for explicitly unbounded intervals in queries.
 (I think i provisioned some code for this in optimizer and custom hbase
 filter, but client doesn't support these constructions per se).
\end_layout

\begin_layout Standard
Is there a clever way of supporting some of HAVING conditions without running
 a full table scan?
\end_layout

\begin_layout Standard
Parallel querying with region server-side preprocessors?
\end_layout

\begin_layout Standard
Support for stringified dimensions.
 probably needs back and forth conversion from string to hashified representatio
n.
 Also, probably functionality to enumerate all distinct members of such
 dimensions.
 More ideas how to represent? Can as well represent as a fixed size type,
 then no hash tricks required.
\end_layout

\begin_layout Standard
Pivoting UI: how to enable them? an MDX minimum dialect (which is an effort
 similar to building a Mondrian or whatever tranlsator)? what minimum subset
 of MDX is needed for such UI? embed our own dialect into jpivot?
\end_layout

\end_body
\end_document
