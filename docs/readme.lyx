#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article
\begin_preamble
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman lmodern
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder false
\pdf_colorlinks false
\pdf_backref false
\pdf_pdfusetitle true
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation skip
\defskip medskip
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
HBase Lattice Quick Start
\end_layout

\begin_layout Author
Dmitriy Lyubimov
\begin_inset Newline newline
\end_inset


\size small
\emph on
dlyubimov@apache.org
\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\begin_inset Newpage cleardoublepage
\end_inset


\end_layout

\begin_layout Section
What it is 
\end_layout

\begin_layout Standard
HBase Lattice is an attempt at BI solution.
 Namely, it is an attempt at building HBase-based incremental OLAP cube.
 
\end_layout

\begin_layout Standard
I scanned surroundings and noticed at least 2 such attempts which for various
 reasons (for most part, maturity and staleness) did not fit our purposes.
 
\end_layout

\begin_layout Standard
Like some MOLAP solutions, HBase-Lattice copes with aggregate queries by
 prebuilding certain cuboids in a cube lattice models.
 
\end_layout

\begin_layout Section
Motivations
\end_layout

\begin_layout Itemize

\series bold
In continuation of 
\begin_inset Quotes eld
\end_inset

Cassandra is OLTP, HBase is OLAP
\begin_inset Quotes erd
\end_inset

 mantra
\series default
.
 HBase is not really an OLAP service out of the door.
 It doesn't support cube models directly.
 There's no query language to use.
 There's no predefined way to update a cube.
 There're no concepts of dimension, hierarchy, measure and fact streams.
\end_layout

\begin_layout Itemize

\series bold
Big underlying fact stream
\series default
.
 Billions, perhaps trillions of facts to process which we want to cope with
 by parallelizing the compilation with the help of MapReduce.
 
\end_layout

\begin_layout Itemize

\series bold
Low query TTLB
\series default
 (especially on Time Series data).
 Our goal was to answer queries over any period of time and whatever other
 slice specifications very quickly with a single hbase table access and
 a very limited amount of iterations in a scan.
 (TTLB <~ 1ms on hbase side, assuming the tablet data is in memory, + whatever
 network overhead).
\end_layout

\begin_layout Itemize

\series bold
Next to realtime data availability for querying
\series default
.
 Use of incremental updates to cuboid projections in the lattice means there's
 no need to recompile the whole cube for the past 90 days or whatever.
 New fact data becomes available within single number of minutes after the
 fact actually happened, as soon as incremental compiler iteration is complete.
 Once compiled, the data remains continuously available unless thrown out
 by HBase during compaction given specified projection TTL parameter.
\end_layout

\begin_layout Itemize

\series bold
Keep stuff within same ecosystem.
 
\series default
Another motivation is to be able to do things within the same resource space
 of HDFS and HBASE one has already invested in.
 While there are definitely other tools out there to try with the same,
 if not greater, capabilities (MongoDB comes to mind), those tools would
 perhaps require their own distinct environment (resources) and perhaps
 bulk data transfer and import.
\end_layout

\begin_layout Section
Differentiating aspects of HBL vs.
 MOLAP, ROLAP and cube lattice model in general
\end_layout

\begin_layout Paragraph
No fact table, no facts kept around.
\end_layout

\begin_layout Standard
We don't keep individual facts around.
 Unlike perhaps with some other approaches, there's no level of indirection
 to query the fact table.
 All projection data is right there, in a cuboid table.
 This provides 2 major benefits: 
\end_layout

\begin_layout Itemize
Low query TTLBs.
 If we are hitting cuboid with precompiled aggregate results, we only need
 to scan a handful of items per request.
 
\end_layout

\begin_layout Itemize
Don't need the space to keep all original facts.
 Depending on the definition of dimensions and hierarchies and the nature
 of incoming fact streams, the space required to keep aggregated projections
 may require several orders of magnitude less space than the original fact
 stream.
\end_layout

\begin_layout Standard
The tradeoff is obviously in that one cannot query individual fact datum.
 It is assumed that facts are kept somewhere else outside HBL tables (and
 they usually are, so no need to mandate data duplication in HBL).
\end_layout

\begin_layout Paragraph
What cuboids are to be compiled is specified manually.
\end_layout

\begin_layout Standard
In the interest of keeping things simple, the model specification explicitly
 lists all cuboids to compile in the cube.
 Consequently, not all aggregated groups are avaialable for querying.
 Working out which cuboids to compile is similar to process where DBA tries
 to figure out which indices to deploy based on use patterns.
 
\end_layout

\begin_layout Standard
New projectsions can be added dynamically to the system.
 Just specify new projections, deploy the model and the compiler component
 will start producing new projections right away.
 (applying new projections over past data retroactively is not easy at this
 point though.
 Pretty much the only way to do that is to drop all existing data and re-compile
 all projections over the entire historical facts again).
\end_layout

\begin_layout Paragraph
Compiler is a Pig codegen.
\end_layout

\begin_layout Standard
The compiler component generates pig script at runtime based on current
 specification of the model.
 (see 
\emph on
sample
\emph default
 module for example how to run these scripts).
\end_layout

\begin_layout Standard
One of the somewhat stale projects on github used similar approach but instead
 of using Apache Pig, that project used python streaming MR.
 But the idea is very similar.
 
\end_layout

\begin_layout Standard
MapReduce compiler allows to do distributed aggregation of the data before
 making updates to hbased-stored cuboids.
 
\end_layout

\begin_layout Paragraph
Querying the data.
\end_layout

\begin_layout Standard
Data querying is available in two ways: 
\end_layout

\begin_layout Itemize
an API query class (not unlike the declarative api way to construct query
 objects in Hibernate), and 
\end_layout

\begin_layout Itemize
a simplistic query language that translates into that api calls to setup
 a query from reporting tools (again, not unlike HQL support in Hibernate).
 
\end_layout

\begin_layout Standard
In either case, a special custom hbase filter is used to allow to skip over
 the rows we are not really interested in, so the scan iterations are kept
 going over mostly relevant facts only.
\end_layout

\begin_layout Section
Quick Howto
\end_layout

\begin_layout Subsection
Specifying a model.
\end_layout

\begin_layout Standard
Model is specified by composing a bunch of java classes representing cube,
 cuboids, hierarchies, dimensions and measures.
 Instead of writing some java code wiring this composition up, it is also
 to use a declarative approach for model definition.
 We use YAML for declarative model definition (see file 
\family typewriter

\begin_inset CommandInset href
LatexCommand href
name "example1.yaml"
target "https://github.com/dlyubimov/HBase-Lattice/blob/dev-0.1.x/sample/src/main/resources/example1.yaml"

\end_inset


\family default
 in the 
\emph on
sample
\emph default
 module of the project for an example of a declarative model defintion).
 We employ 
\emph on

\begin_inset CommandInset href
LatexCommand href
name "SnakeYaml"
target "http://code.google.com/p/snakeyaml/"

\end_inset


\emph default
 project for the purposes of constructing java model objects, see how 
\emph on
SnakeYaml
\emph default
 works with classes and constructors for details of what's going on there.
\end_layout

\begin_layout Subsubsection
Supported dimension types
\end_layout

\begin_layout Itemize

\family typewriter
HexDimension
\family default
.
 This class supports discrete dimensions that are fixed-length byte arrays.
 In hbase composite keys they are translated into ASCII Hex representation
 of such for the sake of simpler readability when using tools like hbase
 shell.
 Hence, the name.
\begin_inset Newline newline
\end_inset

Typically, 
\family typewriter
HexDimension
\family default
 is suitable to represent uniformly-distributed hash IDs or otherwise hash-refer
enced data.
 It accepts java type 
\family typewriter
byte[]
\family default
 and its Pig equivalent (in context of compilation).
 Now also accepts any numbers and converts them to key using Big Endian
 conversion.
 Results returned from queries are always of 
\family typewriter
byte[]
\family default
 type.
\begin_inset Newline newline
\end_inset


\family typewriter
HexDimension
\family default
 is initialized with the value length.
 The fact stream will accept all values shorter or equal in length to the
 one specified in model.
 
\end_layout

\begin_layout Itemize

\family typewriter
SimpleTimeHourHierarchy
\family default
.
 This is a hierarchical dimension to convert 
\family typewriter
GregorianCalendar
\family default
 and/or long values representing ms since epoch in fact streams into hierarchica
l descrete type [ALL].[YEAR-MONTH].[DATE-HOUR].
 I.e.
 the lowest bucket granularity for time series data is 1 hour.
 The continuous member data type for this dimension (when not expressed
 with a hierarchy member) is 
\family typewriter
GregorianCalendar
\family default
, or 
\family typewriter
Long
\family default
 expressing number of milliseconds since epoch (in context of projection
 compilation in pig).
\end_layout

\begin_layout Standard
Dimension types are assumed to be discrete.
 If dimension is computed over a continuous value in the fact stream (such
 as time), it has to be mapped into a discrete one perhaps by means of using
 a hierarchy (e.g.
 current 
\family typewriter
SimpleTimeHourHierarchy 
\family default
copes with continous nature of time by setting up discrete member values
 as hour-long buckets and thus implicitly converts every fact from the fact
 stream into [YEAR-MONTH].[DATE-HOUR] form.
 Obviously, we can make granularity finer and finer so that eventually it
 may go all the way down to a millisecond, and still be able to optimize
 queries with additive and complement scans.
 
\end_layout

\begin_layout Paragraph
Handling NULL values for dimensions in the fact stream.
\end_layout

\begin_layout Standard
At this time, compiler will not accept NULL values for dimensions because
 representation of NULL values is not currently supported in composite hbase
 key structure.
 If it is desirable to consider NULL values as a dimensional coordinate,
 it may be transformed into a reserved non-null value in the compiler's
 preambula.
\end_layout

\begin_layout Subsubsection
Supported measure types
\end_layout

\begin_layout Standard
The measure type per se is currently not limited to a particular type.
 It can be any type.
 However, aggregate functions will only support a particular type.
 If the fact is not of the type an aggregate function would expect during
 compilation, it will be considered NULL without any additional noise.
 
\end_layout

\begin_layout Standard
Most of the aggregate functions like SUM(), COUNT() are supporting facts
 of a numerical type only.
 In the 
\emph on
yaml 
\emph default
model this is defined as 
\family typewriter
SimpleMeasure
\family default
.
 
\end_layout

\begin_layout Standard
Currently, compiler also recognizes pig tuples in a form of 
\begin_inset Formula $\left(x,t\right)$
\end_inset

 tuples where x is a numeric fact and 
\begin_inset Formula $t\in\mathbb{N}_{1}$
\end_inset

 long type representing time of the sample.
 Such facts are converted to an instance of 
\family typewriter
IrregularSample
\family default
 class and subsequently are used with exponentially weighted averaging and
 rate functions.
 To define an irregular fact sampling with time datum, 
\family typewriter
IrregularSampleMeasure
\family default
 must be used in the model
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
See example for the usage details.
\end_layout

\end_inset

.
\end_layout

\begin_layout Paragraph
Handling NULL values for measures in the fact stream.
\end_layout

\begin_layout Standard
Measure values could be NULL in the fact stream.
 It may affect behavior of certain aggregate functions similar to their
 definition in Pig and SQL.
 E.g.
 if slice did not contain non-NULL facts for a measure, SUM() will produce
 NULL but COUNT() will produce 0.
\end_layout

\begin_layout Subsection
Incremental cube compilation
\end_layout

\begin_layout Standard
Cube compilation is done via incremental Pig script dynamically generated
 by compilter component (see sample module for example of the compilation).
 With the current approach, compiler doesn't support any input adapters,
 so it cannot read any standard fact stream sources on its own.
 Instead, it relies on a fragment of the script that reads input into a
 predefined Pig relation, to be supplied.
 This Pig-scripted fragment is called 
\begin_inset Quotes eld
\end_inset

preambula
\begin_inset Quotes erd
\end_inset

 and expected to be supplied via Spring 
\family typewriter
Resource
\family default
 specification.
 
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
Perhaps this only dependency on 
\emph on
Spring
\emph default
 is bad and it is worth considering getting rid of this abstraction; but
 developing a project-specific resource abstraction is probably just as
 equally bad.
 Besides, it facilitates wiring compiler bean up using 
\emph on
Spring
\emph default
, which is what we do.
\end_layout

\end_inset

 The compiler expects fact stream to be put in a Pig relation with a predefined
 name (
\begin_inset Quotes eld
\end_inset

HBL_INPUT
\begin_inset Quotes erd
\end_inset

 by default).
 
\end_layout

\begin_layout Standard
Preambula is also an opportunity to massage fact stream data a little bit
 before handing off to compiler.
 Preambula doesn't suit to host complex preprocessors though because of
 the codegen'd nature of the final script.
 If complex data preprocessing is required, it probably better be done by
 the logging application or a separate preprocessing MR job.
\end_layout

\begin_layout Standard
The requirements for HBL_INPUT relation produced by preambula is as follows:
 
\end_layout

\begin_layout Itemize
It must have all defined dimensions.
 Dimension names used must be the same as in model description.
 Dimension Pig types depend on the dimension class.
\end_layout

\begin_layout Itemize
It must have at least one measure fact (currently, of either long or double
 type only).
 The measure is recognized by having the same name as in model description.
 The scope of measures may be reduced by using exclude/include api on the
 compiler bean (see sample module for an example).
 By default, all measures are expected.
 Using measure scope reduction allows to easily compile in multiple fact
 streams containing different measures and potentially originating in different
 sources (for as long as all dimensions can be inferred for each of them).
 
\end_layout

\begin_layout Subsection
Deploying querying capabilities
\end_layout

\begin_layout Standard
Since custom filters are used, one jar (hbl-0.1.2.jar as of the time of this
 writing) should be deployed to region servers (perhaps with a rolling restart
 afterwards).
 With CDH distribution it turns out it is enought just to drop hbl.jar into
 $HBASE_HOME/lib folder at the region servers.
 Only querying part depends on this, incremental compiler does not depend
 on this.
\end_layout

\begin_layout Subsection
Query API
\end_layout

\begin_layout Standard
TODO
\end_layout

\begin_layout Subsubsection
Supported aggregate functions at this time.
\end_layout

\begin_layout Itemize
SUM().
 This function returns 
\family typewriter
Double
\family default
 in queries.
\end_layout

\begin_layout Itemize
COUNT().
 This function returns 
\family typewriter
Long
\family default
 in queries.
\end_layout

\begin_layout Itemize
Exponentially (or, rather, Canny function) weighted average for facts with
 time-based sampling 
\series bold

\begin_inset Formula $\left(x,t\right)$
\end_inset

 
\series default
as a custom function in the model.
 This function returns instance of 
\family typewriter
OnlineCannyAvgSummarizer
\family default
 containing the weighted sum state.
 It additionally can be used to derive a biased binomial estimate on a slice
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
see metrics doc and example.
\end_layout

\end_inset

.
\end_layout

\begin_layout Itemize
Exponentially (or, rather, Canny function) weighted rate for facts with
 time-based sampling 
\begin_inset Formula $\left(\mbox{count},t\right)$
\end_inset

 as a custom function (see Example and doc)
\end_layout

\begin_layout Itemize
SUM_SQ() sum of squares
\end_layout

\begin_layout Itemize
AVG() 
\end_layout

\begin_layout Itemize
SD() standard deviation (todo: add sample deviation)
\end_layout

\begin_layout Itemize
VAR() standard variance (todo: add sample variance)
\end_layout

\begin_layout Itemize
MIN() 
\end_layout

\begin_layout Itemize
MAX()
\end_layout

\begin_layout Standard
Api allows to add new custom functions easily, it's just all we needed at
 the moment.
\end_layout

\begin_layout Subsection
Querying with a prepared query 
\end_layout

\begin_layout Standard
See the example for how to prepare and use query.
 It is recommended to use prepared query repeatedly to save on parsing it
 into an AST tree.
 (After all, that's what prepared queries are for).
\end_layout

\begin_layout Standard
Approximate current query syntax is (see RFC-822 for the BNF syntax used):
 
\end_layout

\begin_layout Standard

\family typewriter
\begin_inset Box Boxed
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "95col%"
special "none"
height "1in"
height_special "totalheight"
status collapsed

\begin_layout Plain Layout

\family typewriter
'select' select-expr *(',' select-expr) 'from' cube-name [where-clause]
 [group-clause]
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Plain Layout

\family typewriter
select-expr = measure-name / aggr-function [ 'as' alias-name ]
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Plain Layout

\family typewriter
aggregate-function = function-name '(' measure-name ')'
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Plain Layout

\family typewriter
where-clause = 'where' slice-spec *(',' slice-spec)
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Plain Layout

\family typewriter
slice-spec = dimension-name 'in' ('[' / '(') value / '?' [ ',' ( value /
 '?' ) ] (']' / ')')
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Plain Layout

\family typewriter
group-clause = 'group by' dimension-name *(, dimension-name)
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Plain Layout

\family typewriter
measure-name = ID / '?' ; id rules or substitution via a parameter
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Plain Layout

\family typewriter
cube-name = ID / '?' ; id rules or substitution via a parameter
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Plain Layout

\family typewriter
alias-name = ID / '?' ; id rules or substitution via a parameter
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Plain Layout

\family typewriter
function-name = ID / '?' ; id rules or substitution via a parameter
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Plain Layout

\family typewriter
dimension-name = ID / '?' ; id rules or substitution via a parameter
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Plain Layout

\family typewriter
value = ( '
\backslash
'' LITERAL '
\backslash
'' ) / LONG / DOUBLE
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Example:
\end_layout

\begin_layout Standard
\begin_inset Box Boxed
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "95col%"
special "none"
height "1in"
height_special "totalheight"
status open

\begin_layout Plain Layout

\family typewriter
select d1 as dim1, COUNT( m1 ) from Example where d1 in [?], time in [?,?)
 group by d1
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\emph on
Where-clause 
\emph default
is essentially a slice specification.
 Hence specification is imposed on a dimension using opened or closed interval
 semantics.
 E.g.
 [1,3) is a so-called half-open interval which includes between values of
 1 (including) and 3 (excluding).
 The limitation of the 
\emph on
where-clause 
\emph default
is that currently one cannot specify more than one slice specification for
 the same dimension.
 Semantic result of an attempt to specify multiple slices for the same dimension
 is currently undefined.
 
\end_layout

\begin_layout Standard
Aggregating over multidimensional hyperplane (a plane perpendicular to an
 axis and going thru a specific point on that axis) is hence equivalent
 to specifying '
\family typewriter
where dimension in [?]
\family default
'.
\end_layout

\begin_layout Standard
Aggregate functions may return 
\family typewriter
NULL
\family default
 if a measure group had been empty (or consisted only of 
\family typewriter
NULL
\family default
 measure values).
 This semantics is consistent with 
\family typewriter
SUM()
\family default
 and some other aggregate functions semantics in SQL and Pig.
 As a corner case, a measure group might have been empty if reduced measure
 scope was applied during compilation.
 Reduced scope fact stream basically is equivalent to a full fact stream
 having all facts for the excluded measures as 
\family typewriter
NULL
\family default
.
\end_layout

\begin_layout Paragraph
Query limitations.
\end_layout

\begin_layout Itemize
There has to be a cuboid specifying all dimensions in a group clause in
 the leftmost positions.
 Hence, plan optimizer may complain if certain grouping is not possible
 due to lack of suitable cuboid.
\end_layout

\begin_layout Itemize
Complement scan optimizations for hierarchies are not implemented in this
 release (only in our prototype).
\end_layout

\begin_layout Itemize
There's currently no way to run some useful analytic queries like 'select
 COUNT(fact), ip group by ip having COUNT(fact) > 10000'.
 
\end_layout

\begin_layout Itemize
One has to select at least one measure aggregate in the query.
 Technically, there should be no reason why not support a request for dimension
 members satisfying 
\emph on
where-clause
\emph default
 conditions only, but the way it is currently designed, it need a least
 one measure to sum up in an aggregate (even if one doesn't use it).
\end_layout

\begin_layout Itemize
Unlike with MDX, there's no 
\emph on
optimized 
\emph default
way to query a dimension or hierarchy membership.
 Since the system is pretty dynamic, new members might appear at any time
 and at this point we don't keep track of distinct list of them.
 
\emph on
It is possible to query members in a particular slice though, including
 the total cube, 
\emph default
but that would be a full table scan over shortest cuboid still.
\end_layout

\begin_layout Subsection
Complement and additive scan query optimizations.
 
\end_layout

\begin_layout Standard
Scanning a point slice of either dimension or hierarchy is trivial.
\end_layout

\begin_layout Standard
Scanning a range of slices of a dimension is trivial as well (assuming that's
 the last dimension in cuboid spec).
\end_layout

\begin_layout Standard
Scanning a range over a hierarchy is less trivial.
 Hierarchy must support notion of 
\family typewriter
[ALL]
\family default
 member aggregates to be able to produce batch.
 Additionally, hierarchy needs to support optimizing for complement vs.
 union scans.
 (time hierarchies come to mind as a particularly good example of benefiting
 from complement scans).
\end_layout

\begin_layout Standard
Here I'll develop a very simple bit of theory behind additive and complement
 scans.
 
\end_layout

\begin_layout Paragraph
Definition - additive scan-capable aggregate functions.
\end_layout

\begin_layout Standard
Suppose we have a bunch of metrics (facts) 
\begin_inset Formula $\mathbf{M}=\left\{ m_{1},m_{2},...,m_{n}\right\} $
\end_inset

.
 We also consider and aggregate function defined over a fact set, 
\begin_inset Formula $\mbox{aggr}\left(\mathbf{M}\right)$
\end_inset

, which returns a single variable.
 If for any two disjoint subsets 
\begin_inset Formula $\mathbf{M}_{1}$
\end_inset

 and 
\begin_inset Formula $\mathbf{M}_{2}$
\end_inset

:
\begin_inset Formula $\mathbf{M}_{1}\cap\mathbf{M}_{2}=\emptyset$
\end_inset

 also satisfying 
\begin_inset Formula $\mathbf{M}_{1}\cup\mathbf{M}_{2}=\mathbf{M}$
\end_inset

 exists a function 
\begin_inset Formula $\mathbf{add}\left(r_{1},r_{2}\right)$
\end_inset

 such that 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\mbox{aggr}\left(\mathbf{M}\right)=\mbox{add}\left(\mbox{aggr}\left(\mathbf{M}_{1}\right),\mbox{aggr}\left(\mathbf{M}_{2}\right)\right),
\]

\end_inset


\end_layout

\begin_layout Standard
then we call function 
\begin_inset Formula $\mbox{aggr}\left(\cdot\right)$
\end_inset

 
\emph on
additive scan-capable.
\end_layout

\begin_layout Paragraph
Definition - complement scan-capable aggregate functions.
\end_layout

\begin_layout Standard
Similarly, every request can be devised into summing scan over metrict fact
 set 
\begin_inset Formula $\mathbf{M}_{1}$
\end_inset

 and a complement scan over another dataset 
\begin_inset Formula $\mathbf{M}_{2}:\mathbf{M}_{2}\subseteq\mathbf{M}_{1}$
\end_inset

.
 
\end_layout

\begin_layout Standard
Suppose there's an existing aggregating function over metric set 
\begin_inset Formula $\mathrm{aggr}\left(\cdot\right)$
\end_inset

 If there exists a function of two variables 
\begin_inset Formula $\mathbf{complement}\left(r_{1},r_{2}\right)$
\end_inset

 such that for any two fact sets 
\series bold

\begin_inset Formula $\mathbf{M}_{1}$
\end_inset

 
\series default
and 
\begin_inset Formula $\mathbf{M}_{2}$
\end_inset

 satisfying 
\begin_inset Formula $\mathbf{M}_{2}\subseteq\mathbf{M}_{1}$
\end_inset

 the following is true 
\begin_inset Formula 
\begin{eqnarray*}
r & = & \mathrm{aggr}\left(\mathbf{M}_{1}\backslash\mathbf{M}_{2}\right)\\
 & = & \mathrm{\mathbf{complement}}\left(\mathrm{aggr}\left(\mathbf{M}_{1}\right),\mathrm{aggr}\left(\mathbf{M}_{2}\right)\right),
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
then we say that 
\begin_inset Formula $\mathrm{aggr}\left(\cdot\right)$
\end_inset

 is 
\emph on
complement scan-capable.
\end_layout

\begin_layout Standard
Obviously, sum(), count() and avg() could be represented in a way that makes
 them complement scan-capable.
\end_layout

\begin_layout Standard
Complement optimization specifically comes in light for time series scans
 that involve timezone corrections, e.g.
 timezone correction for a month hierarchy with additional complement scan
 on an hour hierarchy.
\end_layout

\begin_layout Standard
Hence, it follows that we should model a hierarchy in a way so that its
 implementation would be able to opimize using complement scans.
 We will require scan additivity of all aggregate functions, but it seems
 that we cannot request complement scan capability of any given aggregate
 function.
 Hence, future complement scan optmization should interrogate functions
 as to whether complement scan optimization is possible.
 
\end_layout

\begin_layout Section
TODOs and FIXMEs
\end_layout

\begin_layout Standard
At this point there's no JDBC provider available (we don't use jdbc; we
 integrate custom datasources directly into our reporting tool.
 Therefore, creating jdbc support ranked very low on our roadmap, but assuming
 there's an external interest in this, it should be an easy enhancement,
 all components are already there for it).
\end_layout

\begin_layout Standard

\series bold
Complement scan optimizations for hierarchies are not in yet
\series default
.
 (but there's a working prototype).
 
\end_layout

\begin_layout Standard
Access to model elements is rudimental.
 Model is exchanged as yaml-serialized string within compiler, which may
 overload pig communication to backend wastefully if model becomes sizeable
 (thousands of measures or dimensions).
 System tables containing model is rudimental as well.
 We seem to employ several dozens of measures in production without noticeable
 problems, but obviously this is a somewhat severe limitation for a 
\begin_inset Quotes eld
\end_inset

big data
\begin_inset Quotes erd
\end_inset

 system.
\end_layout

\begin_layout Standard
Poor selection of aggregate functions.
 Modelling for aggregation functions and supported member types needs more
 thought, it is not flexible enough right now.
\end_layout

\begin_layout Standard
Poor selection of hierarchy and dimension types.
 Add more fine grained time hierarchy.
 Queries don't support specifying hierarchy members in their hierarchical
 inline syntas as in [ALL][2011][JAN] , but only as a continuous value.
 Hierarchical members can be constructed and passed in as parameters though.
\end_layout

\begin_layout Standard
Crosstab output is not formatted as tab (although equivalent data can be
 returned as adjacency list).
\end_layout

\begin_layout Standard
Poor selection of measure types.
 Currently, we are limited to numeric facts in fact stream only.
\end_layout

\begin_layout Standard
Support for explicitly unbounded intervals in queries.
 (I think i provisioned some code for this in optimizer and custom hbase
 filter, but client doesn't support these constructions per se).
\end_layout

\begin_layout Standard
Is there a clever way of supporting some of HAVING conditions without running
 a full table scan?
\end_layout

\begin_layout Standard
Parallel querying with region server-side preprocessors?
\end_layout

\begin_layout Standard
Support for stringified dimensions.
 probably needs back and forth conversion from string to hashified representatio
n.
 Also, probably functionality to enumerate all distinct members of such
 dimensions.
 More ideas how to represent? Can as well represent as a fixed size type,
 then no hash tricks required.
\end_layout

\begin_layout Standard
Pivoting UI: how to enable them? an MDX minimum dialect (which is an effort
 similar to building a Mondrian or whatever tranlsator)? what minimum subset
 of MDX is needed for such UI? embed our own dialect into jpivot?
\end_layout

\end_body
\end_document
